{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import re\n",
    "import string\n",
    "import wget\n",
    "import zipfile\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils as utils\n",
    "\n",
    "from fastai.basic_train import Learner\n",
    "from fastai.train import lr_find\n",
    "from fastai.basic_data import DataBunch\n",
    "from fastai.metrics import accuracy, accuracy_thresh\n",
    "from fastprogress import fastprogress\n",
    "\n",
    "from nn_toolkit.vocab import Vocab, VocabEncoder\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def download_wikitext(local_path: Path) -> None:\n",
    "    if local_path.exists(): return\n",
    "    url = 'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip'\n",
    "    print(f'Downloading {url}')\n",
    "    print(f'Saving to {local_path}')\n",
    "    list(local_path.parents)[0].mkdir(exist_ok=True, parents=True)\n",
    "    wget.download(\n",
    "        url,\n",
    "        str(local_path)\n",
    "    )\n",
    "\n",
    "def get_token_string(zip_path, mode='train'):\n",
    "    assert mode in ('train', 'valid', 'test')\n",
    "    with zipfile.ZipFile(zip_path) as zfo:\n",
    "        dirname = zfo.namelist()[0]\n",
    "        path = Path(f'{dirname}') / f'wiki.{mode}.tokens'\n",
    "        tokens = zfo.open(str(path)).read().decode()\n",
    "        return tokens\n",
    "    \n",
    "def clean_token_string(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\n( =){1,}.*?( =){1,} \\n', '', text)\n",
    "    #text = re.sub('(\\n =|= \\n)', '', text)\n",
    "    text = re.sub(r'[ \\n]{1,}', ' ', text)\n",
    "    return text\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    return text.split()\n",
    "\n",
    "def remove_stopwords(tokens: List[str]) -> List[str]:\n",
    "    return [t for t in tokens if t not in set(stopwords.words('english'))]\n",
    "\n",
    "def prepare_data(zip_path, mode):\n",
    "    token_string = get_token_string(zip_path, mode)\n",
    "    token_string = clean_token_string(token_string)\n",
    "    sents = [remove_stopwords(tokenize(sent) for sent in sent_tokenize(token_string)]\n",
    "    return sents\n",
    "\n",
    "def token_pairs(tokens, window: int):\n",
    "    slow, N = 0, len(tokens)\n",
    "    while slow < len(tokens):\n",
    "        target_token = tokens[slow]\n",
    "        left_edge = max(0, slow-window)\n",
    "        right_edge = min(N, slow+window)\n",
    "        context = tokens[left_edge: slow] + tokens[slow+1:right_edge]\n",
    "        for context_token in context:\n",
    "            yield target_token, context_token\n",
    "        slow += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(utils.data.Dataset):\n",
    "    def __init__(self, sents, vocab_encoder, window: int=2, cbow: bool = False, t: float=1e-3, ns: int=5) -> None:\n",
    "        self.sents = sents\n",
    "        self.vocab_encoder = vocab_encoder\n",
    "        self.window = window\n",
    "        self.cbow = cbow\n",
    "        self.t = t\n",
    "        self.ns = ns\n",
    "        self._prepare_data()\n",
    "\n",
    "    def _prepare_data(self) -> None:\n",
    "        self.encoded_sents = [self.vocab_encoder.encode_sequence(sent, add_specials=False) for sent in self.sents]\n",
    "        self.tokens = [t for sent in self.encoded_sents for t in sent]\n",
    "        #self.char_tokens = [list(t) for sent in self.sents for t in sent]\n",
    "        self._build_p_table()\n",
    "        self.sent_lengths = list(map(len, self.encoded_sents))\n",
    "                \n",
    "    def _build_p_table(self):\n",
    "        self.token_freq = Counter(self.tokens)\n",
    "        self.N = sum(self.token_freq.values())\n",
    "        self.token_p = {k: self.get_p(v/self.N) for k, v in self.token_freq.items()}\n",
    "        self.N_pow = sum([c**0.75 for c in self.token_freq.values()])\n",
    "        self.probs = torch.tensor([\n",
    "            self.token_freq[i]**0.75/self.N_pow for i in range(self.vocab_encoder.size)\n",
    "        ])\n",
    "        \n",
    "    def get_p(self, p: float):\n",
    "        return min( (math.sqrt(p/self.t) + 1) * self.t/p, 1)\n",
    "    \n",
    "    def get_context(self, seq, idx, window):\n",
    "        left_edge = max(0, idx-window)\n",
    "        left = seq[left_edge: idx]\n",
    "        right_edge = min(len(seq), idx+window+1)\n",
    "        right = seq[idx+1: right_edge]\n",
    "        return left+right\n",
    "    \n",
    "    def drop_context(self, seq: List[int]) -> List[int]:\n",
    "        return [idx for idx in seq if np.random.rand()<=self.token_p[idx]]\n",
    "    \n",
    "    def ns_mask(self, label):\n",
    "        mask = torch.empty(self.ns+1, dtype=torch.int64)\n",
    "        old_p = self.probs[label]\n",
    "        self.probs[label] = 0\n",
    "        mask[:-1] = torch.multinomial(self.probs, self.ns, replacement=False)\n",
    "        mask[-1] = label\n",
    "        self.probs[label] = old_p\n",
    "        return mask\n",
    "    \n",
    "    def _negative_sample(self, sample: dict) -> dict:\n",
    "        target, context = sample['target'], sample['context']\n",
    "        sample['context'] = context.repeat(1+self.ns, 1)\n",
    "        fake_targets = self.get_random_target(self.ns * target.size(0)).view(self.ns, -1)\n",
    "        sample['target'] = torch.cat([target.unsqueeze(0), fake_targets])\n",
    "        sample['label'] = torch.zeros(1+self.ns)\n",
    "        sample['label'][0] = 1\n",
    "        return sample\n",
    "    \n",
    "    def get_random_target(self, n):\n",
    "        return torch.multinomial(self.probs, n, replacement=False)\n",
    "        \n",
    "    def _cbow(self, target, context):\n",
    "        return {'context': context, 'target': target}\n",
    "    \n",
    "    def _skipgram(self, target: int, context: List[int]):\n",
    "        return {'context': target, 'target': context}\n",
    "    \n",
    "    def _pad(self, seq: List[int], maxlen: int) -> List[int]:\n",
    "        assert len(seq) <= maxlen\n",
    "        diff = maxlen - len(seq)\n",
    "        return seq + [self.vocab_encoder.pad_index] * diff\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        target = torch.tensor([self.tokens[idx]])\n",
    "        context = self.get_context(self.tokens, idx, self.window)\n",
    "        #context = self.drop_context(context)\n",
    "        context = self._pad(context, 2*self.window)\n",
    "        context = torch.tensor(context)\n",
    "        if self.cbow: sample = self._cbow(target, context)\n",
    "        else: sample = self._skipgram(target, context)\n",
    "        return self._negative_sample(sample)\n",
    "    \n",
    "    def __len__(self):\n",
    "        if getattr(self, '_length', None) is None:\n",
    "            self._length = sum(map(len, self.sents))\n",
    "        return self._length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Sampler(utils.data.Sampler):\n",
    "    def __init__(self, dataset, shuffle: bool=False) -> None:\n",
    "        self.t = dataset.t\n",
    "        self.counter = dataset.token_freq\n",
    "        self.N = sum(self.counter.values())\n",
    "        self.f_table = {i: self.counter[t]/self.N for i, t in enumerate(dataset.tokens)}\n",
    "        self.p = [self.get_p(f) for f in self.f_table.values()]\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def get_p(self, f: float):\n",
    "        return min( (math.sqrt(f/self.t) + 1) * self.t/f, 1)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        idxs = np.arange(len(self))\n",
    "        if self.shuffle: np.random.shuffle(idxs)\n",
    "        for i in idxs:\n",
    "            if self.shuffle and np.random.rand() >= self.p[i]:\n",
    "                continue\n",
    "            else:\n",
    "                yield i\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class TokenDistribution:\n",
    "    def __init__(self, tokens, vocab):\n",
    "        self.counter = Counter(tokens)\n",
    "        self.vocab = vocab\n",
    "        self.vocab_counts = [self.counter[self.vocab.int_to_token[i]] for i in range(self.vocab.size)]\n",
    "        self.N = sum(self.counter.values())\n",
    "        self.pow = 0.75\n",
    "        self.t = 1e-3\n",
    "        \n",
    "    @property\n",
    "    def p(self):\n",
    "        if getattr(self, '_p', None) is None:\n",
    "            self._p = self.compute_distribution()\n",
    "        return self._p\n",
    "    \n",
    "    def get_p(self, f: float):\n",
    "        f += 1e-9\n",
    "        return min( (math.sqrt(f/self.t) + 1) * self.t/f, 1)\n",
    "\n",
    "    def compute_distribution(self):\n",
    "        D = torch.tensor([self.get_p(c/self.N) for c in self.vocab_counts])\n",
    "        return D#/D.sum()\n",
    "    \n",
    "    def resample(self, tokens):\n",
    "        return [t for t in tokens if np.random.rand() <= self.p[self.vocab[t]].item()]\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def collate_batch(batch: List[torch.Tensor]):\n",
    "    targets = torch.cat([sample['target'] for sample in batch], 0)\n",
    "    contexts = torch.cat([sample['context'] for sample in batch], 0)\n",
    "    labels = torch.cat([sample['label'] for sample in batch], 0)\n",
    "    #mask = torch.stack([sample['mask'] for sample in batch])\n",
    "    #labels = targets.view(-1)\n",
    "    #labels = torch.tensor([mask.size(1)-1 for _ in range(mask.size(0))])\n",
    "    return {'context': contexts, 'target': targets}, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.c_layer = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.t_layer = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.eps = 1e-9\n",
    "\n",
    "    def forward(self, X):\n",
    "        context, target = X['context'], X['target']\n",
    "        c_emb = self.c_layer(context)  # (B, c, e)\n",
    "        c_mask = self._pad_mask(context)\n",
    "        c_size = (~c_mask).sum(1, keepdim=True) + self.eps\n",
    "        c_emb = c_emb.sum(1) / c_size\n",
    "        \n",
    "        t_emb = self.t_layer(target)  # (B, t, e)\n",
    "        t_mask = self._pad_mask(target)\n",
    "        t_size = (~t_mask).sum(1, keepdim=True) + self.eps\n",
    "        t_emb = t_emb.sum(1) / t_size\n",
    "        logit = (c_emb * t_emb).sum(1)\n",
    "        return logit\n",
    "\n",
    "    def _pad_mask(self, X):\n",
    "        return X == 0\n",
    "    \n",
    "    def _apply_mask(self, W, idxs):\n",
    "        return W.gather(1, idxs)\n",
    "        B, device = idxs.size(0), logit.device\n",
    "        mask = torch.ones(B, self.W.size(1), device=device)\n",
    "        mask.scatter_(1, idxs, 0)\n",
    "        return logit - 1e9*mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = Path('data/raw/wikitext-2-v1.zip')\n",
    "download_wikitext(text_file)\n",
    "\n",
    "train_sents = prepare_data(text_file, mode='train')\n",
    "val_sents = prepare_data(text_file, mode='valid')\n",
    "\n",
    "train_tokens = [t for sent in train_sents for t in sent]\n",
    "vocab = Vocab(set(train_tokens))\n",
    "token_dist = TokenDistribution(train_tokens, vocab)\n",
    "\n",
    "train_sents = [token_dist.resample(sent) for sent in train_sents]\n",
    "val_sents = [token_dist.resample(sent) for sent in val_sents]\n",
    "\n",
    "vocab_encoder = VocabEncoder(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = True\n",
    "window = 3\n",
    "bs = 4096\n",
    "ns = 5\n",
    "t = 1e-3\n",
    "\n",
    "train_ds = Dataset(train_sents, vocab_encoder, window=window, cbow=cbow, t=t, ns=ns)\n",
    "#train_sampler = Sampler(train_ds, shuffle=True)\n",
    "train_sampler = None\n",
    "train_dl = utils.data.DataLoader(train_ds, batch_size=bs, collate_fn=collate_batch, num_workers=3, sampler=train_sampler)\n",
    "\n",
    "val_ds = Dataset(val_sents, vocab_encoder, window=window, cbow=cbow, t=t, ns=ns)\n",
    "#val_sampler = Sampler(val_ds, shuffle=False)\n",
    "val_sampler = None\n",
    "val_dl = utils.data.DataLoader(val_ds, batch_size=bs, collate_fn=collate_batch, num_workers=3, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch(train_dl, val_dl, device='cuda:0', collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: torch.Size([24576, 6])\n",
      "target: torch.Size([24576, 1])\n",
      "y: torch.Size([24576])\n"
     ]
    }
   ],
   "source": [
    "for X, y in data.valid_dl:\n",
    "    print('context:', X['context'].size())\n",
    "    print('target:', X['target'].size())\n",
    "    print('y:', y.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(vocab_encoder.size, 200)\n",
    "model.to(data.device)\n",
    "learner = Learner(data, model, loss_func=nn.BCEWithLogitsLoss(), opt_func=optim.SGD, metrics=[accuracy_thresh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_thresh</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.097678</td>\n",
       "      <td>1.083030</td>\n",
       "      <td>0.499336</td>\n",
       "      <td>01:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.996174</td>\n",
       "      <td>0.986338</td>\n",
       "      <td>0.499917</td>\n",
       "      <td>01:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.920366</td>\n",
       "      <td>0.910609</td>\n",
       "      <td>0.500326</td>\n",
       "      <td>01:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.859932</td>\n",
       "      <td>0.854259</td>\n",
       "      <td>0.499887</td>\n",
       "      <td>01:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.814375</td>\n",
       "      <td>0.811614</td>\n",
       "      <td>0.498855</td>\n",
       "      <td>01:47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.fit(5, lr=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['king', 'incentive', 'exteriors', 'schubert', 'inactivation', 'copper', 'heliport', 'oppression', 'molds', 'dreams']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    W = model.c_layer.weight\n",
    "    vec1 = W[vocab['king']]\n",
    "    #vec1 = vec1 / torch.norm(vec1)\n",
    "    #norm = torch.norm(W, dim=1) + model.eps\n",
    "    norm = 1\n",
    "    sims = vec1 @ W.t() / norm\n",
    "    most_sim = torch.argsort(sims)\n",
    "    topk = most_sim[-100:].tolist()[::-1]\n",
    "    top_tokens = [vocab_encoder.int_to_token[i] for i in topk]\n",
    "    print(top_tokens[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model = Word2Vec(\n",
    "    train_sents, \n",
    "    size=model.c_layer.embedding_dim, \n",
    "    window=3, \n",
    "    sg=int(not cbow),\n",
    "    negative=ns,\n",
    "    sample=t,\n",
    "    workers=8,\n",
    "    compute_loss=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mayen/miniconda3/envs/wordvec/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('edward', 0.8391004800796509),\n",
       " ('queen', 0.8382359743118286),\n",
       " ('henry', 0.8131130933761597),\n",
       " ('lord', 0.7869649529457092),\n",
       " ('bishop', 0.7864221334457397),\n",
       " ('william', 0.7800500988960266),\n",
       " ('pope', 0.7752436399459839),\n",
       " ('james', 0.7749444246292114),\n",
       " ('elizabeth', 0.774777889251709),\n",
       " ('charles', 0.7678619623184204)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_model.most_similar('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2427534.0"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_model.get_latest_training_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mayen/miniconda3/envs/wordvec/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `min_count` (Attribute will be removed in 4.0.0, use self.vocabulary.min_count instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_model.min_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
